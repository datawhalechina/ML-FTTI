为了定义纯度的概念，我们首先需要思考如何度量不确定性。在生活中，高概率事件代表的不确定性比低概率事件代表的不确定性低，例如：明天太阳从东边升起是必然的，故这个事件的不确定性为0；而明天下雨并不是必然事件，它相比前一个事件具有更高的不确定性。因此，若定义一个可微函数$I(p),p\in [0, 1]$来表示事件$A$发生的概率$p(A)$所代表的不确定性，那么从直觉上应当满足下面三个必要条件，我们将它们称为[信息量](https://en.wikipedia.org/wiki/Information_content)公理。其中，$A_1,...,A_n$为独立事件。

- $I(1)=0$
- $I(p)$关于$p$单调递减
- $I(\prod_{i=1}^np(A_i))=\sum_{i=1}^nI(p(A_i))$

我们已经对信息量公理的前两个条件做出了说明，其第三个条件的含义是：独立事件同时发生的不确定性应当等于这些事件对应发生的不确定性之和，这是非常合理的假设。

根据这些条件，我们容易想到函数

$$
I(p)=a\log_{b}(p)(a(b-1)<0)
$$

符合信息量公理的要求。事实上从充分性的角度而言，它也是能够满足信息量公理的唯一函数。

```{admonition} 定理
设$I(p)$在$[0,1]$上可微，则$I(x)$满足信息量公理的充要条件是 $I(p)=a\log_{b}(p)(a(b-1)<0)$。
```

```{admonition} 证明
当$p\in (0,1]$，此时由导数定义有

$$
\begin{aligned}
I^{'}(p) &= \lim_{\Delta p\rightarrow 0^-}\frac{I(p+\Delta p)-I(p)}{\Delta p} \\
&= \lim_{\Delta p\rightarrow 0^-}\frac{I(\frac{p+\Delta p}{p}\cdot p)-I(p)}{\Delta p} \\
&=\lim_{\Delta p\rightarrow 0^-}\frac{I(\frac{p+\Delta p}{p})}{\Delta p} \\
&= \frac{1}{p}\lim_{\Delta p\rightarrow 0^-}\frac{I(1+\frac{\Delta p}{p})}{\frac{\Delta p}{p}}\\
&= \frac{1}{p}I^{'}(1)
\end{aligned}
$$

两边积分$I(p)=I^{'}(1)\ln p+C,p\in(0,1]$，代入$I(1)=0$得$C=0$，从而

$$
I(p)=I^{'}(1)\ln(p)=\frac{I^{'}(1)}{\log_be}\log_bp
$$

记$a=\frac{I^{'}(1)}{\log_be}$，由单调性可知$I^{'}(1)<0$。当$b>1$时有$\log_be>0$，即$a<0$；当$b<1$时有$\log_be<0$，即$a>0$。因此，符合信息量公理的函数只能是$I(p)=a\log_{b}(p)(a(b-1)<0)$。
```

我们已经知道了信息量对应函数的形式，那么究竟应该如何选取合适的$a$和$b$呢？对于一个以概率为$p$发生的事件$A$而言，我们可以选择一种二进制编码的方式来记录它的信息：当$p=\frac{1}{4}$时，我们可以认为事件$A$的发生本质上是某个随机变量的一种状态，且该随机变量会以等概率出现4种状态，那么我们就可以用00、01、10和11来进行状态信息的记录；当$p=\frac{1}{8}$时，我们需要用000、001、010、011、100、101、110、111的编码来记录。因此，$p$越小则不确定性越高，需要消耗的编码长度越大。此时，编码种类的数量即为$\frac{1}{p}$，事件$A$的二进制编码长度代表的不确定性大小就是$\log_2\frac{1}{p}$。因此，我们可以取$I(p)$中的$a$为$-1$，且取$b$为$2$，用$I(p)=-\log_2(p)$来代表度量不确定性的指标。

先前我们讨论了随机变量取定某个值情况下不确定性的度量，那么如果要定义一个随机变量$Y$的平均不确定性，只需要对这个随机变量按照对应的概率密度分布$p(y)$取期望即可，我们将其称为分布的信息熵（Information Entropy）$H(Y)$（熵是一种反应系统不确定性的指标，由于此处指随机变量信息的不确定性，故称为信息熵），即

$$
H(Y)=\mathbb{E}_{Y}I(p)=\mathbb{E}_{Y\sim p(y)}[-\log_2 p(Y)]
$$

对于定义在有限状态集合$\{y_1,...,y_K\}$上的离散变量而言，对应信息熵的最大值在离散均匀分布时取到，最小值在单点分布时取到。此时，离散信息熵为

$$
H(Y)=-\sum_{k=1}^K p(y_k)\log_2p(y_k)
$$

首先，我们需要定义当$p=0$时$p\log_2p\triangleq 0$，原因在于

$$
\lim \limits_{p\to 0^+} p \log{p} = \lim \limits_{p \to 0^+} \frac{\log p}{1/p} = \lim \limits_{p \to 0^+} \frac{1/p}{-1/p^2}=\lim \limits_{p \to 0^+} -p = 0
$$

离散熵的极值问题是带有约束的极值问题，记$p_k=P(Y=y_k)$和$\mathbf{p}=[p_1,...,p_K]^T$，则约束条件为$\mathbb{1}^T\mathbf{p}=1$，拉格朗日函数为

$$
L(\mathbf{p})=-\mathbf{p}^T\log \mathbf{p} + \lambda (\mathbb{1}^T\mathbf{p}-1)
$$

求偏导数后可解得$\mathbf{p}^*=[\frac{1}{K},...,\frac{1}{K}]^T$，此时$\mathbb{E}_{Y}I(p)=\log K$。

```{admonition} 补充材料
有关向量求导的内容可参考[这个](https://en.wikipedia.org/wiki/Matrix_calculus)wiki页面。
```

对于离散随机变量$X$，由于$p(Y)\in [0,1]$，故$-\log_2p(Y)\geq 0$，从而$\mathbb{E}_{Y}I(p)\geq 0$。注意到对于$\forall k\in \{1,...,K\}$，当$p_k=1$，即$p_{k'}=0(k'\in \{1,...,K\}/k)$时，$H(X)=0$。因此，离散信息熵的最小值为0且在单点分布时取到。由于$\mathbf{p}^*$是极值问题的唯一解，因此离散熵的最大值为$\log K$且在离散均匀分布时取到。

这些结论都是与直觉高度吻合的。单点分布的取值被唯一确定，因此随机变量的不确定性为0；在给定状态集合数量下，分布越是均匀，则随机变量的不确定性越是大；当$K\rightarrow +\infty$时，离散均匀分布的熵为无穷大，一个合理的解释是：随着取值集合元素数量的增加，我们对每一个元素平均而言的信息把握程度就越少，不确定性就越大。