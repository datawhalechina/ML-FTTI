## 5. 带权样本的训练

C4.5算法还能够处理带权重的样本：设所有节点的样本权重为$\mathbf{w}^{(D)}=(w^{(D)}_1,...,w^{(D)}_n)$，其中$n$为全体样本的个数；当前分裂节点$N$的样本权重为$\mathbf{w}^{(N)}=({w}^{(N)}_1,...,{w}^{(N)}_{n_{N}})$，其中$n_{N}$为当前节点的样本个数；当前节点按照特征$\mathbf{y}$分裂的第$m$个子节点权重为$\mathbf{w}^{(m)}=(w^{(m)}_1,...,w^{(m)}_{n_{m}})$，其中$n_m$为该子节点的样本个数。

此时可得到带权的信息增益为

$$
G_N(X,Y)=-\sum_{i=1}^K\tilde{p}(x_k)\log \tilde{p}(x_k)+\sum_{m=1}^M\frac{\sum_{i=1}^{n_{m}}{w}^{(m)}_{i}}{\sum_{i=1}^{n_{N}}{w}^{(N)}_{i}}\tilde{p}(y_m)\tilde{p}(x_k\vert y_m)\log \tilde{p}(x_k\vert y_m)
$$

先前提到了min\_impurity\_decrease参数，$\frac{D_N}{D_{all}}G^{max}_N(X,Y)$的值会与此阈值进行对比以决定节点$N$是否分裂。对于带权样本，我们可以将相对最大信息增益修正为

$$
\frac{\sum_{i=1}^{n_{N}}{w}^{(N)}_{i}}{\sum_{i=1}^{n}{w}^{(D)}_{i}}G^{max}_N(X,Y)
$$

此处，我们需要注意到当样本的权重被设为全1时，加权信息增益和修正的阈值对比值与原来的定义完全一致，具有越高权重的样本就越容易对模型的分裂决策产生影响。

当样本带有权重时，加权信息增益定义为

$$
G_{\mathbf{w}}^{MSE}(X,Y)=-\sum_{i=1}^{N}(y^{(D)}_i-\bar{y}^{(D)})^2+\frac{\sum_{i=1}^{N_L}w^{(N)}_i}{\sum_{i=1}^{N}w^{(N_L)}_i}\sum_{i=1}^{N_L}(y^{(L)}_i-\bar{y}^{(L)})^2+\frac{\sum_{i=1}^{N_R}w^{(N)}_i}{\sum_{i=1}^{N}w^{(N_R)}_i}\sum_{i=1}^{N_R}(y^{(R)}_i-\bar{y}^{(R)})^2
$$

$$
G_{\mathbf{w}}^{MAE}(X,Y)=-\sum_{i=1}^{N}\vert y^{(D)}_i-\tilde{y}^{(D)}\vert+\frac{\sum_{i=1}^{N_L}w^{(N)}_i}{\sum_{i=1}^{N}w^{(N_L)}_i}\sum_{i=1}^{N_L}\vert y^{(L)}_i-\tilde{y}^{(L)}\vert+\frac{\sum_{i=1}^{N_R}w^{(N)}_i}{\sum_{i=1}^{N}w^{(N_R)}_i}\sum_{i=1}^{N_R}\vert y^{(R)}_i-\tilde{y}^{(R)}\vert
$$



10. sklearn提供了class\_weight参数来处理非平衡样本。设每个类别的样本数量为$n_1,...,n_K$，第$i$个样本的类别、样本权重和类别权重分别为$k$、$w_i$和$w^c_i$。当class\_weight值是形式为\{class\_label: class\_weight\}的字典时，样本权重被调整为$w_i\cdot w^c_i$；当class\_weight值是字符串“balanced”，样本权重被调整为$w_i\cdot \frac{\sum_{k'=1}^K n_{k'}}{K\cdot n_k}$；否则$w_i$不变。现有样本$x_1$、$x_2$和$x_3$的样本权重为$[20,30,10]$，类别分别是$0$、$0$和$1$，且给定class\_weight=\{0:40, 1:60\}，请计算调整后的样本权重。

请实现参数与sklearn一致的DecisionTreeClassifier类，其成员函数包括fit、predict和predict\_proba，同时需给出feature\_importances\_指标。
    - predict\_proba返回的是测试样本所在叶节点的各类别比例。
    - feature\_importances\_指每个特征的重要性，对于某个特征而言，其特征重要性等于决策树中根据该特征分裂而产生的相对信息增益之和。


请实现SAMME、SAMME.R和Adaboost.R2算法。

- 设 $$\begin{aligned}
    y' &= \sup \{ y'\big| \sum_{m\in \{m\vert y_m\leq y'\}}\alpha^{(m)} \\ &\leq 0.5 \sum_{m=1}^M\alpha^{(m)}\}
    \end{aligned}$$ ，它和定义中给出的$y$值一定相等吗？若不一定请举出反例。

## 4. XGBoost的优化设计

### 查询优化

### 系统优化

## 6. LightGBM的优化设计

### 直方图算法

直方图算法（XGBoost从2017年开始也实现了几乎一致的直方图算法，但分割点算法保持不变，这也是XGBoost中tree_method设置为approx和hist的区别，详情见[这个commit](https://github.com/dmlc/xgboost/issues/1950)）、

### 投票并行
LightGBM对传统的数据并行策略和特征并行策略都进行了改进，并通过投票并行方法将数据并行的机器通讯开销降至常数复杂度，有关这方面的内容请参考[文档](https://lightgbm.readthedocs.io/en/latest/Features.html#optimization-in-distributed-learning)及其附录中分布式系统设计的相关论文。

类别变量

### 直方图算法

关于直方图算法的分箱细节资料很少，这里主要参考了[马冬什么的知乎文章](https://zhuanlan.zhihu.com/p/85053333)。

```{admonition} 特征并行和数据并行
在分裂节点构造子节点直方图时，不同的特征可以并行处理，并且在处理同一个特征时对于梯度和与箱子内样本数的累计计算也可以并行处理。
```